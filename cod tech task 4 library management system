# Import necessary libraries
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score, davies_bouldin_score
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
import seaborn as sns
import os
from google.colab import drive

# Mount Google Drive
drive.mount('/content/drive')

# Set the correct file path (assuming your dataset is in 'library management' folder)
file_path = '/content/drive/MyDrive/library management/library_dataset.csv'

# Step 1: Check if the dataset exists; if not, create it
if not os.path.exists(file_path):
    print("Dataset not found. Creating dataset...")
    # Generate assumed data
    data = {
        'BookID': np.arange(1, 101),
        'Genre': np.random.choice(['Fiction', 'Non-Fiction', 'Sci-Fi', 'Romance', 'Mystery'], 100),
        'Pages': np.random.randint(100, 1000, 100),
        'Ratings': np.round(np.random.uniform(1, 5, 100), 1),
        'BorrowFrequency': np.random.randint(1, 50, 100),
        'LibraryBranch': np.random.choice(['Central', 'West', 'East', 'North', 'South'], 100),
    }

    # Create a DataFrame
    df = pd.DataFrame(data)

    # Save the dataset to Google Drive
    df.to_csv(file_path, index=False)
    print(f"Dataset created successfully at {file_path}!")
else:
    # Load the dataset from Google Drive
    df = pd.read_csv(file_path)
    print("Dataset loaded successfully!")

# Step 2: Inspect the dataset
print(df.head())
print(df.info())

# Step 3: Data Preprocessing
# Select features for clustering
features = df[['Genre', 'Pages', 'Ratings', 'BorrowFrequency', 'LibraryBranch']]

# One-hot encode categorical columns and scale numeric columns
preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), ['Pages', 'Ratings', 'BorrowFrequency']),
        ('cat', OneHotEncoder(), ['Genre', 'LibraryBranch'])
    ]
)

processed_data = preprocessor.fit_transform(features)

# Step 4: Determine the optimal number of clusters using the Elbow Method
inertia = []
for k in range(2, 10):
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(processed_data)
    inertia.append(kmeans.inertia_)

plt.figure(figsize=(8, 5))
plt.plot(range(2, 10), inertia, marker='o')
plt.title('Elbow Method')
plt.xlabel('Number of Clusters')
plt.ylabel('Inertia')
plt.show()

# Step 5: Perform clustering with the chosen number of clusters
optimal_clusters = 3  # Update based on the Elbow plot
kmeans = KMeans(n_clusters=optimal_clusters, random_state=42)
labels = kmeans.fit_predict(processed_data)
df['Cluster'] = labels

# Step 6: Evaluate the Clustering
silhouette_avg = silhouette_score(processed_data, labels)
db_index = davies_bouldin_score(processed_data, labels)
print(f'Silhouette Score: {silhouette_avg}')
print(f'Davies-Bouldin Index: {db_index}')

# Step 7: Visualize the Clusters
# Reduce dimensions for visualization
pca = PCA(n_components=2)
reduced_data = pca.fit_transform(processed_data)

# Plot clusters
plt.figure(figsize=(8, 5))
sns.scatterplot(x=reduced_data[:, 0], y=reduced_data[:, 1], hue=df['Cluster'], palette='viridis')
plt.title('Cluster Visualization')
plt.xlabel('PCA Component 1')
plt.ylabel('PCA Component 2')
plt.legend(title='Cluster')
plt.show()
